{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f05be5f",
   "metadata": {},
   "source": [
    "# \"제목 !!\"\n",
    "> \"요약!! \"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: In Chan\n",
    "- categories: [jupyter, deep learning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a146d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "# import Engine\n",
    "from utils import *\n",
    "import utils\n",
    "import math\n",
    "from torch.autograd import Function\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model import Extractor, Classifier\n",
    "from Engine import Engine\n",
    "import time\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8035a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    log, logclose = create_logger(log_filename=log_filename,display=False)\n",
    "    log(f'Trial Number = {trial.number}')\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    in_ch = 2\n",
    "    in_size=66\n",
    "    Activation_name_generator = trial.suggest_categorical(\"Activate Function for Generator\", [\"ReLU\", \"LeakyReLU\", \"ELU\",\"Tanh\"])\n",
    "    Activation_generator = getattr(nn, Activation_name_generator)\n",
    "    \n",
    "    for i in range(5):\n",
    "        out_ch = [int(96),int(256),int(384),int(384),int(256)]\n",
    "        padding= trial.suggest_int(\"padding{}\".format(i), 0, 5)\n",
    "        kernel= trial.suggest_int(\"kernel{}\".format(i), 1, 6)\n",
    "        stride= trial.suggest_int(\"stride{}\".format(i), 1, 5)\n",
    "        Max_stride = trial.suggest_int(\"Max_stride{}\".format(i), low=1, high=2)\n",
    "        # Dilation = trial.suggest_int(\"Dilation{}\".format(i),1, 3)\n",
    "\n",
    "        layers.append(nn.Conv1d(in_ch, out_ch[i], kernel, stride, padding))\n",
    "        layers.append(nn.BatchNorm1d(out_ch[i]))\n",
    "        layers.append(Activation_generator())\n",
    "        if i==1 or 2 or 5 :\n",
    "            layers.append(nn.MaxPool1d(2,Max_stride))\n",
    "        \n",
    "        in_ch=out_ch[i]\n",
    "\n",
    "        in_size=((in_size-kernel+2*padding)/stride +1)\n",
    "        # in_size=((in_size+2*padding-Dilation*(kernel-1))/stride +1)\n",
    "\n",
    "        if in_size.__class__ == float :\n",
    "            in_size = float(math.floor(in_size))\n",
    "        \n",
    "        if i == 1 or 2 or 5 : \n",
    "            if Max_stride == 2 : \n",
    "                in_size=(in_size-2)/2+1\n",
    "                if in_size.__class__ == float :\n",
    "                    in_size = float(math.floor(in_size))\n",
    "            if Max_stride == 1 : \n",
    "                in_size=(in_size-2)+1\n",
    "                if in_size.__class__ == float :\n",
    "                    in_size = float(math.floor(in_size))\n",
    "\n",
    "        \n",
    "        if in_size <=10 :\n",
    "            break\n",
    "    layers.append(nn.Flatten())  \n",
    "\n",
    "    \n",
    "    # layers.append(nn.Linear(int(in_size*out_ch[i]), num_fc_node, bias=True))\n",
    "    Feature_extractor = nn.Sequential(*layers)\n",
    "    \n",
    "    ## Classifier\n",
    "    class_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    layers =[]\n",
    "    \n",
    "    Activation_name_classifier = trial.suggest_categorical(\"Activate Function for Classifier\", [\"ReLU\", \"LeakyReLU\", \"ELU\",\"Tanh\"])\n",
    "    Activation_classifier = getattr(nn, Activation_name_classifier)\n",
    "    \n",
    "    node_in_size = int(in_size*out_ch[i])\n",
    "    for i in range(class_layers):\n",
    "        out_size= trial.suggest_int(\"cl_out_size{}\".format(i), 10, 1000)\n",
    "        layers.append(nn.Linear(node_in_size, out_size, bias=True))\n",
    "        layers.append(Activation_classifier())\n",
    "        node_in_size = out_size\n",
    "\n",
    "    layers.append(nn.Linear(out_size,6))\n",
    "    Classifier_layer = nn.Sequential(*layers)\n",
    "\n",
    "    class CNN(torch.nn.Module):\n",
    "        \n",
    "        def __init__(self, Feature_extractor ):\n",
    "            super(CNN, self).__init__()\n",
    "            self.Feature_extractor = Feature_extractor\n",
    "            \n",
    "        \n",
    "        def forward(self, x):\n",
    "            features = self.Feature_extractor(x)\n",
    "            return features\n",
    "\n",
    "    Feature_extractor = CNN(Feature_extractor)\n",
    "\n",
    "    class DNN(torch.nn.Module):        \n",
    "        def __init__(self, Classifier ):\n",
    "            super(DNN, self).__init__()\n",
    "            self.Classifier = Classifier\n",
    "        def forward(self, x):\n",
    "            Pred = self.Classifier(x)\n",
    "            return Pred\n",
    "\n",
    "    Feature_extractor = CNN(Feature_extractor)\n",
    "    Classifier1 = DNN(Classifier_layer)\n",
    "    Classifier2 = DNN(Classifier_layer)\n",
    "\n",
    "    log('#####################################################################')\n",
    "    log('############################### Model ################################')\n",
    "    for key, value in trial.params.items():\n",
    "        log(\"{}: {}\".format(key, value))\n",
    "    log('##############################################################################')\n",
    "    log('############################### Hyper Prameters #################################')\n",
    "    log(f'Generator Activation : {Activation_name_generator}')\n",
    "    log(f'Classifier Activation : {Activation_name_classifier}')\n",
    "    logclose()\n",
    "\n",
    "    return Feature_extractor , Classifier1, Classifier2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc718a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # log, logclose = create_logger(log_filename=log_filename)\n",
    "    \n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    DEVICE = 'cuda'\n",
    "    EPOCH = 30 \n",
    "    Feature_Extractor, Classifier1, Classifier2 = define_model(trial)\n",
    "    Feature_Extractor = Feature_Extractor.to(DEVICE)\n",
    "    Classifier1 = Classifier1.to(DEVICE)\n",
    "    Classifier2 = Classifier2.to(DEVICE)\n",
    "\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"Adagrad\",\"Adadelta\"])\n",
    "\n",
    "    lr_E = trial.suggest_float(\"lr\", 1e-7, 1e-3, log=True)\n",
    "    lr_C = trial.suggest_float(\"lr2\", 1e-7, 1e-3, log=True)\n",
    "    \n",
    "    optimizer_E = getattr(optim, optimizer_name)(Feature_Extractor.parameters(), lr=lr_E)\n",
    "    optimizer_C1 = getattr(optim, optimizer_name)(Classifier1.parameters(), lr=lr_C)\n",
    "    optimizer_C2 = getattr(optim, optimizer_name)(Classifier2.parameters(), lr=lr_C)\n",
    "    \n",
    "    Criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    scheduler_name = trial.suggest_categorical(\"scheduler\", [\"LambdaLR\", \"StepLR\", \"CosineAnnealingLR\",'None'])\n",
    "    if scheduler_name == \"LambdaLR\" :\n",
    "        scheduler_E = optim.lr_scheduler.LambdaLR(optimizer = optimizer_E,lr_lambda = lambda epoch:0.95 **epoch)\n",
    "        scheduler_C1 = optim.lr_scheduler.LambdaLR(optimizer = optimizer_C1,lr_lambda = lambda epoch:0.95 **epoch)\n",
    "        scheduler_C2 = optim.lr_scheduler.LambdaLR(optimizer = optimizer_C2,lr_lambda = lambda epoch:0.95 **epoch)\n",
    "    if scheduler_name == \"StepLR\" :\n",
    "        scheduler_E = optim.lr_scheduler.StepLR(optimizer = optimizer_E,step_size = 6 ,gamma=0.75)\n",
    "        scheduler_C1 = optim.lr_scheduler.StepLR(optimizer = optimizer_C1,step_size = 6 ,gamma=0.75)\n",
    "        scheduler_C2 = optim.lr_scheduler.StepLR(optimizer = optimizer_C2,step_size = 6 ,gamma=0.75)\n",
    "    if scheduler_name == \"CosineAnnealingLR\" :\n",
    "        scheduler_E = optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer_E,T_max=5)\n",
    "        scheduler_C1 = optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer_C1,T_max=5)\n",
    "        scheduler_C2 = optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer_C2,T_max=5)\n",
    "    if scheduler_name == \"None\" :\n",
    "        scheduler_E = optim.lr_scheduler.LambdaLR(optimizer = optimizer_E,lr_lambda = lambda epoch:1 **epoch)\n",
    "        scheduler_C1 = optim.lr_scheduler.LambdaLR(optimizer = optimizer_C1,lr_lambda = lambda epoch:1 **epoch)\n",
    "        scheduler_C2 = optim.lr_scheduler.LambdaLR(optimizer = optimizer_C2,lr_lambda = lambda epoch:1 **epoch)\n",
    "        \n",
    "    N = trial.suggest_int(\"Training Number of Generator\", 1, 10) # Training Number of Generator \n",
    "    #############################################################\n",
    "\n",
    "    Best_Loss = np.inf\n",
    "    Best_Accuracy_10 = 0\n",
    "    Best_Accuracy_30 = 0\n",
    "\n",
    "    log, logclose = create_logger(log_filename=log_filename,display=False)\n",
    "    log(f'Generator Learning Rate = {lr_E}')\n",
    "    log(f'Classifier Learning Rate = {lr_C}')\n",
    "    log(f'Optimizer Name : {optimizer_name}')\n",
    "    log(f'Scheduler Name : {scheduler_name}')\n",
    "    log(f'Training Number of Generator : {N}')\n",
    "    log('############################### Hyper Prameters #################################')\n",
    "    log('##############################################################################')\n",
    "    log('\\n')\n",
    "    # print('Start Learning')\n",
    "    Eng = Engine(optimizer_E,optimizer_C1,optimizer_C2,Feature_Extractor,Classifier1,Classifier2,DEVICE)\n",
    "    for epoch in range(EPOCH) : \n",
    "        Loss_A , Loss_B, Loss_C = Eng.train(source_loader, target_loader,N)\n",
    "        ACC1_10 , ACC2_10, Test_Loss_1_10, Test_Loss_2_10 = Eng.test(test_loader_10)\n",
    "        ACC1_30 , ACC2_30, Test_Loss_1_30, Test_Loss_2_30 = Eng.test(test_loader_30)\n",
    "        \n",
    "        scheduler_E.step()\n",
    "        scheduler_C1.step()\n",
    "        scheduler_C2.step()\n",
    "\n",
    "        log, logclose = create_logger(log_filename=log_filename)\n",
    "        log(f'============= Epoch_{epoch} =============')\n",
    "        log(f'Loss A = {Loss_A:0.4f} , Loss B = {Loss_B:0.4f} , Loss C = {Loss_C:0.4f}')\n",
    "        log(f'Accuracy 1 - 10% SL = {ACC1_10*100:0.2f}, Accuracy 2 - 10% SL = {ACC2_10*100:0.2f} ')\n",
    "        log(f'Test Loss 1 - 10% SL = {Test_Loss_1_10:0.4f}, Test Loss 2 - 10% SL = {Test_Loss_2_10:0.4f} ')\n",
    "        log(f'Accuracy 1 - 30% SL = {ACC1_30*100:0.2f}, Accuracy 2 - 30% SL = {ACC2_30*100:0.2f} ')\n",
    "        log(f'Test Loss 1 - 30% SL = {Test_Loss_1_30:0.4f}, Test Loss 2 - 30% SL = {Test_Loss_2_30:0.4f} ')\n",
    "\n",
    "        if max(ACC1_10,ACC2_10) > Best_Accuracy_10 : \n",
    "            Best_Accuracy_10 = max(ACC1_10,ACC2_10)\n",
    "            log(f'--------------------------------Best Accuracy_10 = {Best_Accuracy_10 * 100}--------------------------------')\n",
    "            ACC_10 = [ACC1_10,ACC2_10]\n",
    "            ACC_30 = [ACC1_30,ACC2_30]\n",
    "            index = ACC_10.index(Best_Accuracy_10)\n",
    "            Best_Accuracy_30 = ACC_30[index]\n",
    "            log(f'--------------------------------Best Accuracy_30 = {Best_Accuracy_30 * 100}--------------------------------')\n",
    "\n",
    "        logclose()\n",
    "        trial.report(Best_Accuracy_10,Best_Accuracy_30, epoch)\n",
    "            # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "################################################################\n",
    "    return Best_Accuracy_10,Best_Accuracy_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "793ee1ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please set either 'minimize' or 'maximize' to direction. You can also set the corresponding `StudyDirection` member.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c0864e2a2642>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# study = optuna.create_study(direction=[\"maximize\",\"maximize\"],sampler=optuna.samplers.TPESampler())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"maximize\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"maximize\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m# study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-learning-20\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36mcreate_study\u001b[1;34m(storage, sampler, pruner, study_name, direction, load_if_exists, directions)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     ):\n\u001b[0;32m   1128\u001b[0m         raise ValueError(\n\u001b[1;32m-> 1129\u001b[1;33m             \u001b[1;34m\"Please set either 'minimize' or 'maximize' to direction. You can also set the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;34m\"corresponding `StudyDirection` member.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Please set either 'minimize' or 'maximize' to direction. You can also set the corresponding `StudyDirection` member."
     ]
    }
   ],
   "source": [
    "name = time.asctime(time.localtime(time.time()))\n",
    "name = name.replace(' ','_').replace(':','_')\n",
    "log_dir = f'./log/{name}'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_filename = os.path.join(log_dir, 'train.log')\n",
    "\n",
    "# Data Load\n",
    "folds = make_folds(5)\n",
    "S_train_data , S_train_label , _ , _ = source_Load_data(folds)\n",
    "_ , _ , T_test_data , T_test_label = target_Load_data(folds)\n",
    "\n",
    "test_data_10 , test_label_10 =test_Load_data_10(folds)\n",
    "test_data_30 , test_label_30 =test_Load_data_30(folds)\n",
    "\n",
    "source_set = utils.CustomDataset(data=S_train_data, label = S_train_label)\n",
    "source_loader=DataLoader(source_set,batch_size=32,shuffle=True,drop_last=False)\n",
    "\n",
    "target_set = utils.CustomDataset(data=T_test_data, label = T_test_label)\n",
    "target_loader = DataLoader(target_set,batch_size=32,shuffle=True,drop_last=False)\n",
    "\n",
    "test_set_10 = utils.CustomDataset(data=test_data_10, label = test_label_10)\n",
    "test_loader_10 = DataLoader(test_set_10,batch_size=32,shuffle=True,drop_last=False)\n",
    "\n",
    "test_set_30 = utils.CustomDataset(data=test_data_30, label = test_label_30)\n",
    "test_loader_30 = DataLoader(test_set_30,batch_size=32,shuffle=True,drop_last=False)\n",
    "\n",
    "# study = optuna.create_study(direction=[\"maximize\",\"maximize\"],sampler=optuna.samplers.TPESampler())\n",
    "study = optuna.create_study(direction=[\"maximize\",\"maximize\"])\n",
    "# study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=5000)\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)\n",
    "plot_intermediate_values(study)\n",
    "plot_parallel_coordinate(study)\n",
    "plot_parallel_coordinate(study, params=[\"bagging_freq\", \"bagging_fraction\"])\n",
    "plot_contour(study)\n",
    "plot_contour(study, params=[\"bagging_freq\", \"bagging_fraction\"])\n",
    "plot_slice(study)\n",
    "plot_slice(study, params=[\"bagging_freq\", \"bagging_fraction\"])\n",
    "plot_param_importances(study)\n",
    "optuna.visualization.plot_param_importances(\n",
    "    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n",
    ")\n",
    "plot_edf(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090bc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
