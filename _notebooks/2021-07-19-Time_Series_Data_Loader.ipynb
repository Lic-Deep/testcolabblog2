{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# \"시계열 데이터 Segmentation\"\n",
    "> \"Vibration data\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [coding, jupyter]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 시계열 데이터 Segmentation & Data Loader 만들기\n",
    "- 아래의 코드는 2021 PHMAP Data Challenge Regression Task에서 사용한  Data Loader의 코드임\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv 파일 이름 불러오기\n",
    "file_name = glob.glob('C:/Users/dldls/Desktop/따오기/210708_New/raw_data/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일 읽고, 전처리해서 원하는 값들을 data_set 리스트에 추가\n",
    "data_set = []\n",
    "for i in range(len(file_name)):\n",
    "    data = pd.read_csv(file_name[i])\n",
    "    data = np.concatenate([np.array(data)[:,1].reshape(len(data),1),np.array(data)[:,2].reshape(len(data),1)],axis=1)\n",
    "    data = data.transpose(1,0)\n",
    "    data_set.append(np.array(data))"
   ]
  },
  {
   "source": [
    "- 아래 코드는 긴 시계열 데이터를 10등분하고 , 10등분한 data를 다시 10등분해 , 앞의 9개는 학습 데이터, 뒤의 1개는 테스트 데이터로 사용\n",
    "- 위와 같이 학습/테스트 데이터를 나눈 이유는 한 개의 파일이라도 구간에 따라서 진동 데이터의 경향이 매우 다르기 때문."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devide_data(data) : \n",
    "    split = np.array_split(data,10) # split = data 10등분\n",
    "    for i in range(10) : \n",
    "        split_split = np.array_split(split[i],10) # split split은 10등분한 데이터를 다시 10등분해서 \n",
    "        # 앞 9개는 train\n",
    "        for j in range(9) : \n",
    "            if j == 0 : \n",
    "                tr = split_split[0]\n",
    "            else : \n",
    "                tr=np.hstack([tr,split_split[j]])\n",
    "        # 뒤 1개는 validation \n",
    "        val = np.hstack([split_split[-1]])\n",
    "        if i ==0 : \n",
    "            train = tr\n",
    "            valid = val\n",
    "        else : \n",
    "            train = np.hstack([train,tr])\n",
    "            valid = np.hstack([valid,val])\n",
    "    return train, valid"
   ]
  },
  {
   "source": [
    "- 아래는 각 파일별로 train / validation 데이터를 나눈 후, 각각 저장하는 코드 \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_motor = []\n",
    "valid_motor = []\n",
    "train_skrew = []\n",
    "valid_skrew = []\n",
    "for i in range(len(file_name)): \n",
    "    data_motor = data_set[i][0]\n",
    "    data_skrew = data_set[i][1]\n",
    "    tr_motor , val_motor = devide_data(data_motor)\n",
    "    tr_skrew , val_skrew = devide_data(data_skrew)\n",
    "    if i == 0 : \n",
    "        train_motor = tr_motor\n",
    "        valid_motor = val_motor\n",
    "        train_skrew = tr_skrew\n",
    "        valid_skrew = val_skrew\n",
    "    else : \n",
    "        train_motor = np.hstack([train_motor,tr_motor])\n",
    "        valid_motor = np.hstack([valid_motor,val_motor])\n",
    "        train_skrew = np.hstack([train_skrew,tr_skrew])\n",
    "        valid_skrew = np.hstack([valid_skrew,val_skrew])"
   ]
  },
  {
   "source": [
    "- 앞에서 잘 자른 학습/테스트 데이터를 이용해 Pytorch model에서 사용 할 Data Loader를 만드는 코드 \n",
    "- 여기서는 긴 시퀀스의 시계열 데이터를 15개 간격으로 잘라서 각각의 데이터를 형성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,data_motor,data_skrew):\n",
    "        self.data_motor=data_motor\n",
    "        self.data_skrew=data_skrew\n",
    "    def __len__(self):\n",
    "        return (len(self.data_motor)-4216)//15\n",
    "    def __getitem__(self,idx):\n",
    "        x1=torch.FloatTensor(self.data_motor[idx*15:idx*15+3162]).view([1,3162]) # motor\n",
    "        x2=torch.FloatTensor(self.data_skrew[idx*15:idx*15+3162]).view([1,3162]) # skrew\n",
    "        x3=torch.FloatTensor(self.data_motor[idx*15+3162:idx*15+4216]).view([1,1054]) # motor\n",
    "        label=torch.FloatTensor(self.data_skrew[idx*15+3162:idx*15+4216]).view([1,1054]) # skrew\n",
    "        return x1,x2,x3,label\n"
   ]
  },
  {
   "source": [
    "- train과 valid data를 이용해서 각각 train_loader 와 val_loader를 형성 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=CustomDataset(train_motor,train_skrew)\n",
    "val_set=CustomDataset(valid_motor,valid_skrew)\n",
    "train_loader = DataLoader(train_set,batch_size = 32, shuffle=True)\n",
    "val_loader = DataLoader(val_set,batch_size = 32, shuffle=True)"
   ]
  }
 ]
}